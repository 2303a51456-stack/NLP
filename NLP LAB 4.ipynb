{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMPlMITEBp5u",
        "outputId": "6b8595a7-51f5-4711-f7ba-4262034e8b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (42115, 5)\n",
            "Columns available: Index(['title', 'pubDate', 'guid', 'link', 'description'], dtype='object')\n",
            "\n",
            "Sample cleaned text:\n",
            " 0    the ukrainian president says the country will ...\n",
            "1    jeremy bowen was on the frontline in irpin as ...\n",
            "2    one of the worlds biggest fertiliser firms say...\n",
            "3    the parents of the manchester arena bombings y...\n",
            "4    consumers are feeling the impact of higher ene...\n",
            "Name: clean_desc, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 frequent tokens (NLTK):\n",
            " [('says', 4561), ('world', 2030), ('bbc', 2011), ('people', 1989), ('england', 1922), ('first', 1905), ('new', 1894), ('say', 1676), ('cup', 1486), ('uk', 1462)]\n",
            "\n",
            "Top 10 frequent lemmas (spaCy):\n",
            " [('say', 5463), ('england', 2313), ('year', 2250), ('world', 2170), ('bbc', 2026), ('people', 2021), ('win', 1918), ('new', 1898), ('cup', 1501), ('day', 1500)]\n",
            "\n",
            "Top 10 stems (PorterStemmer):\n",
            " [('say', 6397), ('bbc', 2408), ('england', 2338), ('year', 2251), ('world', 2170), ('peopl', 2021), ('first', 1909), ('new', 1894), ('win', 1747), ('uk', 1682)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "import spacy\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/bbc_news.csv\")\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Columns available:\", df.columns)\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "df['clean_desc'] = df['description'].apply(clean_text)\n",
        "print(\"\\nSample cleaned text:\\n\", df['clean_desc'].head())\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "all_tokens = []\n",
        "\n",
        "for text in df['clean_desc']:\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
        "    all_tokens.extend(filtered_tokens)\n",
        "\n",
        "\n",
        "token_freq = Counter(all_tokens)\n",
        "top10_tokens = token_freq.most_common(10)\n",
        "print(\"\\nTop 10 frequent tokens (NLTK):\\n\", top10_tokens)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "lemmas = []\n",
        "for doc in nlp.pipe(df['clean_desc'].tolist(), disable=[\"parser\", \"ner\"]):\n",
        "    for token in doc:\n",
        "        if not token.is_stop and token.is_alpha and len(token.lemma_) > 1:\n",
        "            lemmas.append(token.lemma_.lower())\n",
        "\n",
        "\n",
        "lemma_freq = Counter(lemmas)\n",
        "top10_lemmas = lemma_freq.most_common(10)\n",
        "print(\"\\nTop 10 frequent lemmas (spaCy):\\n\", top10_lemmas)\n",
        "\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(token) for token in all_tokens]\n",
        "stem_freq = Counter(stems)\n",
        "print(\"\\nTop 10 stems (PorterStemmer):\\n\", stem_freq.most_common(10))\n"
      ]
    }
  ]
}