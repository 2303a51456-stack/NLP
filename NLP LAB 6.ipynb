{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nSoJap_9Drq",
        "outputId": "a1663958-41c9-48c4-cd94-7f58d8d43989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.5828 - loss: 0.6611 - val_accuracy: 0.7931 - val_loss: 0.4542\n",
            "Epoch 2/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8430 - loss: 0.3608 - val_accuracy: 0.7898 - val_loss: 0.4506\n",
            "Epoch 3/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8964 - loss: 0.2639 - val_accuracy: 0.7882 - val_loss: 0.5032\n",
            "Epoch 4/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9344 - loss: 0.1912 - val_accuracy: 0.7800 - val_loss: 0.5606\n",
            "Epoch 5/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9484 - loss: 0.1543 - val_accuracy: 0.7783 - val_loss: 0.6640\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.6226 - loss: 0.6466 - val_accuracy: 0.8112 - val_loss: 0.4329\n",
            "Epoch 2/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.8787 - loss: 0.2971 - val_accuracy: 0.8062 - val_loss: 0.4382\n",
            "Epoch 3/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9464 - loss: 0.1645 - val_accuracy: 0.7931 - val_loss: 0.5121\n",
            "Epoch 4/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - accuracy: 0.9723 - loss: 0.0903 - val_accuracy: 0.7603 - val_loss: 0.5981\n",
            "Epoch 5/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9751 - loss: 0.0817 - val_accuracy: 0.8046 - val_loss: 0.5804\n",
            "Epoch 1/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.6109 - loss: 0.6501 - val_accuracy: 0.8177 - val_loss: 0.4258\n",
            "Epoch 2/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 76ms/step - accuracy: 0.8724 - loss: 0.3157 - val_accuracy: 0.7997 - val_loss: 0.4382\n",
            "Epoch 3/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - accuracy: 0.9323 - loss: 0.1930 - val_accuracy: 0.7833 - val_loss: 0.5206\n",
            "Epoch 4/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - accuracy: 0.9620 - loss: 0.1192 - val_accuracy: 0.7816 - val_loss: 0.5838\n",
            "Epoch 5/5\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.9715 - loss: 0.0859 - val_accuracy: 0.7635 - val_loss: 0.7277\n",
            "\n",
            "MLP Results:\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Accuracy: 0.7688772160210111\n",
            "Precision: 0.7422512234910277\n",
            "Recall: 0.7010785824345146\n",
            "F1-score: 0.7210776545166403\n",
            "\n",
            "CNN Results:\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "Accuracy: 0.7728168089297439\n",
            "Precision: 0.7370892018779343\n",
            "Recall: 0.7257318952234206\n",
            "F1-score: 0.7313664596273292\n",
            "\n",
            "Bi-LSTM Results:\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
            "Accuracy: 0.7491792514773473\n",
            "Precision: 0.7076205287713841\n",
            "Recall: 0.7010785824345146\n",
            "F1-score: 0.7043343653250774\n",
            "\n",
            "Logistic Regression Results:\n",
            "Accuracy: 0.8030203545633617\n",
            "Precision: 0.8273921200750469\n",
            "Recall: 0.6795069337442219\n",
            "F1-score: 0.7461928934010152\n",
            "\n",
            "SVM Results:\n",
            "Accuracy: 0.7774130006565988\n",
            "Precision: 0.7574750830564784\n",
            "Recall: 0.7026194144838213\n",
            "F1-score: 0.7290167865707434\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')   # <-- add this\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv(\"/content/train.csv\")\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
        "X = tfidf.fit_transform(df[\"clean_text\"]).toarray()\n",
        "y = df[\"target\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "mlp_model = models.Sequential([\n",
        "    layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "mlp_model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(df[\"clean_text\"])\n",
        "X_seq = tokenizer.texts_to_sequences(df[\"clean_text\"])\n",
        "X_pad = pad_sequences(X_seq, maxlen=50)\n",
        "\n",
        "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
        "    X_pad, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "cnn_model = models.Sequential([\n",
        "    layers.Embedding(input_dim=10000, output_dim=128, input_length=50),\n",
        "    layers.Conv1D(128, 5, activation='relu'),\n",
        "    layers.GlobalMaxPooling1D(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train_seq, y_train_seq, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "\n",
        "bilstm_model = models.Sequential([\n",
        "    layers.Embedding(input_dim=10000, output_dim=128, input_length=50),\n",
        "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
        "    layers.GlobalMaxPooling1D(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "bilstm_model.fit(X_train_seq, y_train_seq, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, deep=True):\n",
        "    if deep:\n",
        "        y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "    else:\n",
        "        y_pred = model.predict(X_test)\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "    print(\"F1-score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "print(\"\\nMLP Results:\")\n",
        "evaluate_model(mlp_model, X_test, y_test, deep=True)\n",
        "\n",
        "print(\"\\nCNN Results:\")\n",
        "evaluate_model(cnn_model, X_test_seq, y_test_seq, deep=True)\n",
        "\n",
        "print(\"\\nBi-LSTM Results:\")\n",
        "evaluate_model(bilstm_model, X_test_seq, y_test_seq, deep=True)\n",
        "\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "log_reg.fit(X_train, y_train)\n",
        "print(\"\\nLogistic Regression Results:\")\n",
        "evaluate_model(log_reg, X_test, y_test, deep=False)\n",
        "\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_train, y_train)\n",
        "print(\"\\nSVM Results:\")\n",
        "evaluate_model(svm, X_test, y_test, deep=False)"
      ]
    }
  ]
}